{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "environment": {
      "name": "tf2-gpu.2-4.mnightly-2021-02-12-debian-10-test",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:mnightly-2021-02-12-debian-10-test"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "CovidXray.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/icanardahan/COVID-19-Prediction-from-X-Ray-Pictures/blob/main/CovidXray.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stone-slovenia"
      },
      "source": [
        "# Libraries\n",
        "import os, glob, time, random\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow.keras.layers as Layers"
      ],
      "id": "stone-slovenia",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soviet-sarah"
      },
      "source": [
        "# Helper Functions\n",
        "def setupMatplotLib():\n",
        "    plt.rc('figure', figsize=(10,7))\n",
        "    plt.rc('font', size=14)\n",
        "    plt.rc('axes', titlesize=18)     # fontsize of the axes title\n",
        "    plt.rc('axes', labelsize=18)     # fontsize of the x and y labels\n",
        "    plt.rc('xtick', labelsize=14)    # fontsize of the tick labels\n",
        "    plt.rc('ytick', labelsize=14)    # fontsize of the tick labels\n",
        "    plt.rc('legend', fontsize=12)    # legend fontsize\n",
        "    plt.rc('figure', titlesize=20)   # fontsize of the figure title\n",
        "setupMatplotLib()\n",
        "\n",
        "def plot_history(histories, vertical=False, otherkey='accuracy'):\n",
        "    plt.rc('figure', figsize=(14,8))\n",
        "    if vertical:\n",
        "        p,r = 2,1\n",
        "    else:\n",
        "        p,r = 1,2 \n",
        "    anyPlots = False\n",
        "    k=1\n",
        "    for name, history in histories:\n",
        "        if 'val_loss' in history.history.keys():\n",
        "            anyPlots = True\n",
        "            \n",
        "            ax = plt.subplot(p,r,1)\n",
        "            val = ax.plot(history.epoch, history.history['val_loss'],\n",
        "                           '--', label=name.title()+' Val')\n",
        "            ax.plot(history.epoch, history.history['loss'], color=val[0].get_color(),\n",
        "                     label=name.title()+' Train')\n",
        "        elif 'loss' in history.history.keys():\n",
        "            anyPlots = True\n",
        "            \n",
        "            ax = plt.subplot(p,r,1)\n",
        "            ax.plot(history.epoch, history.history['loss'], label=name.title()+' Train')\n",
        "            \n",
        "        if 'val_' + otherkey in history.history.keys():\n",
        "            k = 2\n",
        "            anyPlots = True\n",
        "            \n",
        "            ax = plt.subplot(p,r,2)\n",
        "            val = ax.plot(history.epoch, history.history['val_' + otherkey],\n",
        "                           '--', label=name.title()+' Val')\n",
        "            ax.plot(history.epoch, history.history[otherkey], color=val[0].get_color(),\n",
        "                     label=name.title()+' Train')\n",
        "        elif otherkey in history.history.keys():\n",
        "            k = 2\n",
        "            anyPlots = True\n",
        "            \n",
        "            ax = plt.subplot(p,r,2)\n",
        "            ax.plot(history.epoch, history.history[otherkey], label=name.title()+' Train')\n",
        "        plt.rc('figure', figsize=(10,7))\n",
        "\n",
        "    if anyPlots:\n",
        "        for i in range(1,k+1):\n",
        "            if i == 1:\n",
        "                key = histories[0][1].model.loss\n",
        "            else:\n",
        "                key = otherkey\n",
        "            ax = plt.subplot(p,r,i)\n",
        "            ax.set_xlabel('Epochs')\n",
        "            ax.set_ylabel(key.replace('_',' ').title())\n",
        "            ax.legend()\n",
        "        plt.tight_layout()\n",
        "        \n",
        "def plotClassCounts(countDict):\n",
        "    plt.figure(figsize=(12,8))\n",
        "    plt.bar(range(len(countDict)), countDict.values())\n",
        "    plt.xticks(range(len(countDict)),countDict.keys())\n",
        "    plt.ylabel('Sayılar')\n",
        "    plt.xlabel('Sınıflar')\n",
        "\n",
        "# We assume that all files in the folder are images\n",
        "def plotRandomImages(path):\n",
        "    classNames = ['COVID-19','normal','pneumonia']\n",
        "    images = []\n",
        "    for i, cn in enumerate(classNames):\n",
        "        imageNames = imagesC19 = os.listdir(os.path.join(path,cn))\n",
        "        filepath = random.choice(imageNames)\n",
        "        images.append(Image.open(os.path.join(path,cn,filepath)))\n",
        "    plt.figure(figsize=(18,6))\n",
        "    for i in range(3):\n",
        "        plt.subplot(1,3,i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(images[i], cmap=plt.cm.binary_r)\n",
        "        plt.xlabel(classNames[i])\n",
        "    plt.show()"
      ],
      "id": "soviet-sarah",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "featured-factor"
      },
      "source": [
        "# Training and validation set to start with the same random kernel\n",
        "seed = int(time.time())\n",
        "\n",
        "# The pictures we have are 256x256. We train it as 128x128 so that the training is a little faster\n",
        "imsize = (128, 128)\n",
        "\n",
        "# Whether we will replace the data with different transformations\n",
        "# Generally, this works if the data is low, but does not seem necessary for this problem.\n",
        "augment = False \n",
        "\n",
        "datadir = 'data'\n",
        "\n",
        "if augment:\n",
        "    \n",
        "    # We select enough to fit in the memory, you may need to choose different where you use\n",
        "    batch_size = 256\n",
        "    \n",
        "    traingGen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rotation_range=3,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        rescale=1/255.,\n",
        "        validation_split=0.2,\n",
        "        dtype='float16'\n",
        "    )\n",
        "    \n",
        "    testGen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rescale=1/255.,\n",
        "        dtype='float16'\n",
        "    ) \n",
        "    \n",
        "    trainData = traingGen.flow_from_directory(\n",
        "        directory = os.path.join(datadir, 'train'),\n",
        "        color_mode=\"grayscale\",\n",
        "        batch_size=batch_size,\n",
        "        target_size=imsize,\n",
        "        shuffle=True,\n",
        "        subset='training',\n",
        "        seed = seed,\n",
        "        class_mode=\"sparse\"\n",
        "    )\n",
        "    \n",
        "    valData = traingGen.flow_from_directory(\n",
        "        directory = os.path.join(datadir, 'train'),\n",
        "        color_mode=\"grayscale\",\n",
        "        batch_size=batch_size,\n",
        "        target_size=imsize,\n",
        "        shuffle=True,\n",
        "        subset='validation',\n",
        "        seed = seed,\n",
        "        class_mode=\"sparse\"\n",
        "    )\n",
        "    \n",
        "    testData = testGen.flow_from_directory(\n",
        "        directory = os.path.join(datadir, 'test'),\n",
        "        color_mode=\"grayscale\",\n",
        "        batch_size=batch_size,\n",
        "        target_size=imsize,\n",
        "        class_mode=\"sparse\",\n",
        "        shuffle = False\n",
        "    )\n",
        "    \n",
        "else:\n",
        "    \n",
        "    # We select enough to fit in the memory, you may need to choose different where you use\n",
        "    batch_size = 128\n",
        "    \n",
        "    trainData = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        directory = os.path.join(datadir, 'train'),\n",
        "        color_mode=\"grayscale\",\n",
        "        batch_size=batch_size,\n",
        "        image_size=imsize,\n",
        "        shuffle=True,\n",
        "        validation_split=0.2,\n",
        "        subset='training',\n",
        "        seed = seed\n",
        "    )\n",
        "\n",
        "    valData = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        directory = os.path.join(datadir, 'train'),\n",
        "        color_mode=\"grayscale\",\n",
        "        batch_size=batch_size,\n",
        "        image_size=imsize,\n",
        "        shuffle=True,\n",
        "        validation_split=0.2,\n",
        "        subset='validation',\n",
        "        seed = seed\n",
        "    )\n",
        "\n",
        "    testData = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        directory = os.path.join(datadir, 'test'),\n",
        "        color_mode=\"grayscale\",\n",
        "        batch_size=batch_size,\n",
        "        image_size=imsize,\n",
        "        shuffle = False\n",
        "    )"
      ],
      "id": "featured-factor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "early-princess"
      },
      "source": [
        "# Let's look at the class numbers\n",
        "\n",
        "# If we wanted to count from data, but because we know the numbers, we pass\n",
        "#if augment:\n",
        "#    yTrain = trainData.classes\n",
        "#    yTest  = testData.classes\n",
        "#else:\n",
        "#    yTrain = np.concatenate([y for x, y in trainData], axis=0)\n",
        "#    yTest  = np.concatenate([y for x, y in testData], axis=0)\n",
        "\n",
        "# Training and test clusters were separated from the data as 90-10. While training, we will separate 20% of 90% as validation.\n",
        "counts = {'train':{'COVID-19':1593,'normal':7966,'pneumonia':5462},\n",
        "          'test': {'COVID-19':177, 'normal': 885,'pneumonia': 607}}\n",
        "\n",
        "# Class proportions are obviously not equal\n",
        "train_ratios = np.array(list(counts['train'].values()))/sum(counts['train'].values())\n",
        "\n",
        "print('Class Proportions:',train_ratios)"
      ],
      "id": "early-princess",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "annoying-hacker"
      },
      "source": [
        "plotClassCounts(counts['train'])"
      ],
      "id": "annoying-hacker",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usual-mineral"
      },
      "source": [
        "# Random Examples\n",
        "plotRandomImages('data/train')"
      ],
      "id": "usual-mineral",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liberal-pipeline"
      },
      "source": [
        "Now it is time to set up the model. We provide a very small model to start with, but this isn't enough"
      ],
      "id": "liberal-pipeline"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sufficient-victor"
      },
      "source": [
        "# It's up to you to improve it!\n",
        "def baseCnnClassifier(optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    \n",
        "    # A typical convolutional group\n",
        "    model.add(Layers.Conv2D(filters = 16, kernel_size=(5,5), strides = 1, padding='same'))\n",
        "    model.add(Layers.Activation('relu'))\n",
        "    model.add(Layers.Conv2D(filters = 16, kernel_size=(5,5), strides = 1, padding='same'))\n",
        "    model.add(Layers.Activation('relu')) \n",
        "    model.add(Layers.BatchNormalization()) \n",
        "    model.add(Layers.MaxPool2D(pool_size=(2,2),strides=2))\n",
        "    \n",
        "    model.add(Layers.Conv2D(filters=32,kernel_size=(3,3), strides = 1, padding='same'))\n",
        "    model.add(Layers.Activation('relu'))\n",
        "    model.add(Layers.Conv2D(filters=32,kernel_size=(3,3), strides = 1, padding='same'))\n",
        "    model.add(Layers.Activation('relu'))\n",
        "    model.add(Layers.BatchNormalization())\n",
        "    model.add(Layers.MaxPool2D(pool_size=(2,2),strides=2))\n",
        "    \n",
        "    # We flatten it to attach it to the output layer\n",
        "    model.add(Layers.GlobalAveragePooling2D())\n",
        "    \n",
        "    model.add(Layers.Dense(3,activation='softmax', dtype='float32'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "id": "sufficient-victor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blocked-drinking"
      },
      "source": [
        "# Lets create a model\n",
        "baseModel = baseCnnClassifier(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005))\n",
        "\n",
        "# To end early in case of overfitting\n",
        "earlyStop = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
        "\n",
        "# Record the best performing model according to our validation cluster.\n",
        "modelSave = keras.callbacks.ModelCheckpoint('./checkpoint',monitor='val_accuracy', \n",
        "                                            save_best_only=True, save_weights_only=True,\n",
        "                                            mode='max',)\n",
        "epochs = 20\n",
        "\n",
        "baseHist = baseModel.fit(trainData,\n",
        "                         epochs = epochs, verbose = 1,\n",
        "                         # We will choose a model according to the validation data and stop the training.\n",
        "                         validation_data=valData,\n",
        "                         callbacks = [earlyStop, modelSave],                         \n",
        "                         # Class rates were not equal, we are increasing its weight\n",
        "                         class_weight = {0: 4, 1: 1, 2: 1}\n",
        "                        )\n",
        "\n",
        "baseModel.summary()"
      ],
      "id": "blocked-drinking",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "expensive-utilization"
      },
      "source": [
        "plot_history([('COVID',baseHist)]) "
      ],
      "id": "expensive-utilization",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "varied-conspiracy"
      },
      "source": [
        "# We load the weights that give the best validation result.\n",
        "baseModel.load_weights('./checkpoint')\n",
        "#print('Training performance', baseModel.evaluate(trainData))\n",
        "\n",
        "#We test it with a cluster that we don't use in education and model selection.\n",
        "print('Test performance', baseModel.evaluate(testData))"
      ],
      "id": "varied-conspiracy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "strange-pilot"
      },
      "source": [
        "Look at the results in more detail."
      ],
      "id": "strange-pilot"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fancy-papua"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix, ConfusionMatrixDisplay\n",
        "ypred = np.argmax(baseModel.predict(testData), axis=-1)\n",
        "if augment:\n",
        "    ytrue = testData.classes\n",
        "else:\n",
        "    ytrue = np.concatenate([y for x, y in testData], axis=0)\n",
        "print(classification_report(ytrue, ypred))"
      ],
      "id": "fancy-papua",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sapphire-verse"
      },
      "source": [
        "cm = confusion_matrix(ytrue, ypred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['COVID-19','Normal','Pneumonia'])\n",
        "disp.plot(cmap='Blues')"
      ],
      "id": "sapphire-verse",
      "execution_count": null,
      "outputs": []
    }
  ]
}